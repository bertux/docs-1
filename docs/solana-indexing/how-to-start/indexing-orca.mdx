---
title: "Indexing Orca Whirlpool"
description: >-
  Indexing Orca Whirlpool transactions on Solana
sidebar_position: 10
---

# Indexing the Orca DEX

In this step-by-step tutorial we will look into a squid that gets data about [Orca Exchange](https://www.orca.so/) NFTs, their transfers and owners from the [Solana blockchain](https://solana.com).

Pre-requisites: Node.js, [Subsquid CLI](/squid-cli/installation), Docker.

## Download the project

Begin by retrieving the template and installing the dependencies:
```bash
git clone https://github.com/subsquid-labs/solana-example
cd solana-example
npm i
```

## Interfacing with the contract IDL

First, we inspect which data is available for indexing. For Solana programs written with [Anchor framework](https://www.anchor-lang.com/), the metadata descrbing the shape of the instructions, transactions and contract variables is distributed as an [Interface Definition Language](https://www.quicknode.com/guides/solana-development/anchor/what-is-an-idl) (IDL) JSON file. For many popular programs IDL files are published on [Solscan](https://solscan.io) (as in the case of the Whirlpool program). Subsquid provides a [tool](/sdk/reference/typegen/state-queries) for retrieving contract ABIs from Etherscan-like APIs and generating the boilerplate for retrieving and decoding the data. For the contract of interest, this can be done with
```bash
npx squid-evm-typegen src/abi 0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d#bayc
```
Here, `src/abi` is the destination folder and the `bayc` suffix sets the base name for the generated file.

Checking out the generated `src/abi/bayc.ts` file we see all events and contract functions listed in the ABI. Among them there is the `Transfer` event:
```typescript
export const events = {
    ...
    Transfer: new LogEvent<([from: string, to: string, tokenId: bigint] & {from: string, to: string, tokenId: bigint})>(
        abi, '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef'
    ),
}
```
Reading about [elsewhere](https://eips.ethereum.org/EIPS/eip-721) we learn that it is emitted every time an NFT changes hand and that its [logs](https://medium.com/mycrypto/understanding-event-logs-on-the-ethereum-blockchain-f4ae7ba50378) contain the addresses of both involved parties, as well the unique ID of the token. This is the data we need, so we proceed to configure our squid to retrieve it.

## Configuring the data source

A "data source" is the Node.js component that defines where to get the data and what data should we get. Together with the processor they are responsible for retrieving filtered blockchain data from a specialized data lake (a [Subsquid Network](/subsquid-network/overview) dataset), transforming it and saving the result to a destination of choice. 
To configure the data source to retrieve the `swap` instruction of the Whirlpool program, we initialize it like this:
```typescript title="src/main.ts"
// ...
import {run} from '@subsquid/batch-processor'
import {augmentBlock} from '@subsquid/solana-objects'
import {DataSourceBuilder, SolanaRpcClient} from '@subsquid/solana-stream'
import {TypeormDatabase} from '@subsquid/typeorm-store'
import assert from 'assert'
import * as tokenProgram from './abi/token-program'
import * as whirlpool from './abi/whirpool'
import {Exchange} from './model'


const dataSource = new DataSourceBuilder()
    // Provide Subsquid Network Gateway URL.
    .setGateway('https://v2.archive.subsquid.io/network/solana-mainnet')
    .setRpc(process.env.SOLANA_NODE == null ? undefined : {
        client: new SolanaRpcClient({
            url: process.env.SOLANA_NODE,
            // rateLimit: 100 // requests per sec
        }),
        strideConcurrency: 10
    })

    .setBlockRange({from: 240_000_000})
    .setFields({
        block: { // block header fields
            timestamp: true
        },
        transaction: { // transaction fields
            signatures: true
        },
        instruction: { // instruction fields
            programId: true,
            accounts: true,
            data: true
        },
        tokenBalance: { // token balance record fields
            preAmount: true,
            postAmount: true,
            preOwner: true,
            postOwner: true
        }
    })
    //
    .addInstruction({
        // select instructions, that:
        where: {
            programId: [whirlpool.programId], // where executed by Whirlpool program
            d8: [whirlpool.swap.d8], // have first 8 bytes of .data equal to swap descriptor
            ...whirlpool.swap.accountSelection({ // limiting to USDC-SOL pair only
                whirlpool: ['7qbRF6YsyGuLUVs6Y1q64bdVrfe4ZcUUz1JRdoVNUJnm']
            }),
            isCommitted: true // where successfully committed
        },
        // for each instruction selected above
        // make sure to also include:
        include: {
            innerInstructions: true, // inner instructions
            transaction: true, // transaction, that executed the given instruction
            transactionTokenBalances: true, // all token balance records of executed transaction
        }
    }).build()
// ...
```

Here,
* `'https://v2.archive.subsquid.io/network/solana-mainnet'` is the address for the public Subsquid Network dataset endpoint for Solana mainnet. Check out [Subsquid reference pages](/subsquid-network/reference) for lists of public dataset endpoints for all supported networks.
* `'process.env.SOLANA_NODE'` is an environment variable pointing at a public RPC endpoint we chose to use in this example. When an endpoint is available, the processor will begin ingesting data from it once it reaches the highest block available within Subsquid Network.
* `240_000_000` first Solana block currently indexed by Subsquid.
* The argument of [`addInstruction()`](/solana-indexing/sdk/solana-batch/instructions) is a set of filters that tells the processor to retrieve all swap instructions in the Whirlpool program with discriminator matching the hash of the `<namespace>:<instruction>` of the swap instruction.
* The argument of [`setFields()`](/solana-indexing/sdk/solana-batch/field-selection) specifies the exact data we need on every event to be retrieved. 
See [configuration](/solana-indexing/sdk/solana-batch) for more options.

## Decoding the event data

The other part of processor configuration is the callback function used to process batches of the filtered data, the [batch handler](/sdk/overview/#processorrun). It is typically defined at the `processor.run()` call at `src/main.ts`, like this:
```typescript
processor.run(db, async (ctx) => {
    // data transformation and persistence code here
})
```
Here,
* `db` is a [`Database` implementation](/sdk/resources/persisting-data/overview/) specific to the target data sink. We want to store the data in a PostgreSQL database and present with a GraphQL API, so we provide a [`TypeormDatabase`](/sdk/resources/persisting-data/typeorm/) object here.
* `ctx` is a [batch context](/sdk/overview/#batch-context) object that exposes a batch of data retrieved from Subsquid Network or a RPC endpoint (at `ctx.blocks`) and any data persistence facilities derived from `db` (at `ctx.store`).

Batch handler is where the raw on-chain data is decoded, transformed and persisted. This is the part we'll be concerned with for the rest of the tutorial.

We begin by defining a database and starting the data processing:
```typescript title="src/main.ts"
const database = new TypeormDatabase()


// Now we are ready to start data processing
run(dataSource, database, async ctx => {
    // Block items that we get from `ctx.blocks` are flat JS objects.
    //
    // We can use `augmentBlock()` function from `@subsquid/solana-objects`
    // to enrich block items with references to related objects and
    // with convenient getters for derived data (e.g. `Instruction.d8`).
    let blocks = ctx.blocks.map(augmentBlock)

    let exchanges: Exchange[] = []

    for (let block of blocks) {
        for (let ins of block.instructions) {
            // https://read.cryptodatabytes.com/p/starter-guide-to-solana-data-analysis
            if (ins.programId === whirlpool.programId && ins.d8 === whirlpool.swap.d8) {
                let exchange = new Exchange({
                    id: ins.id,
                    slot: block.header.slot,
                    tx: ins.getTransaction().signatures[0],
                    timestamp: new Date(block.header.timestamp * 1000)
                })

                assert(ins.inner.length == 2)
                let srcTransfer = tokenProgram.transfer.decode(ins.inner[0])
                let destTransfer = tokenProgram.transfer.decode(ins.inner[1])

                let srcBalance = ins.getTransaction().tokenBalances.find(tb => tb.account == srcTransfer.accounts.source)
                let destBalance = ins.getTransaction().tokenBalances.find(tb => tb.account === destTransfer.accounts.destination)

                let srcMint = ins.getTransaction().tokenBalances.find(tb => tb.account === srcTransfer.accounts.destination)?.preMint
                let destMint = ins.getTransaction().tokenBalances.find(tb => tb.account === destTransfer.accounts.source)?.preMint

                assert(srcMint)
                assert(destMint)

                exchange.fromToken = srcMint
                exchange.fromOwner = srcBalance?.preOwner || srcTransfer.accounts.source
                exchange.fromAmount = srcTransfer.data.amount

                exchange.toToken = destMint
                exchange.toOwner = destBalance?.postOwner || destBalance?.preOwner || destTransfer.accounts.destination
                exchange.toAmount = destTransfer.data.amount

                exchanges.push(exchange)
            }
        }
    }

    await ctx.store.insert(exchanges)
})
```
This goes through all the instructions in the block, verifies that they indeed are `swap` instruction from the Whirlpool program and decodes the data of each inner instruction.
Then it retrieves the transaction from the decoded inner instruction and source and destination accounts.
The decoding is done with the `tokenProgram.transfer.decode` function from the Typescript IDL provided in the project.

At this point the squid is ready for its first test run. Execute
```bash
npx tsc
sqd process
docker compose up -d 
npx squid-typeorm-migration apply
node -r dotenv/config lib/main.js
```
You can verify that the data is being stored in the database by running
```bash
docker exec "$(basename "$(pwd)")-db-1" psql -U postgres -c "SELECT * FROM exchange"
```
The full code can be found [here](https://github.com/subsquid-labs/solana-example).

